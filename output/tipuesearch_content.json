{"pages":[{"title":"Are ‘you' just inside your skin or is your smartphone part of you?","text":"In November 2017, a gunman entered a church in Sutherland Springs in Texas, where he killed 26 people and wounded 20 others. He escaped in his car, with police and residents in hot pursuit, before losing control of the vehicle and flipping it into a ditch. When the police got to the car, he was dead. The episode is horrifying enough without its unsettling epilogue. In the course of their investigations, the FBI reportedly pressed the gunman's finger to the fingerprint-recognition feature on his iPhone in an attempt to unlock it. Regardless of who's affected, it's disquieting to think of the police using a corpse to break into someone's digital afterlife. Most democratic constitutions shield us from unwanted intrusions into our brains and bodies. They also enshrine our entitlement to freedom of thought and mental privacy. That's why neurochemical drugs that interfere with cognitive functioning can't be administered against a person's will unless there's a clear medical justification. Similarly, according to scholarly opinion , law-enforcement officials can't compel someone to take a lie-detector test, because that would be an invasion of privacy and a violation of the right to remain silent. But in the present era of ubiquitous technology, philosophers are beginning to ask whether biological anatomy really captures the entirety of who we are. Given the role they play in our lives, do our devices deserve the same protections as our brains and bodies? After all, your smartphone is much more than just a phone. It can tell a more intimate story about you than your best friend. No other piece of hardware in history, not even your brain, contains the quality or quantity of information held on your phone: it ‘knows' whom you speak to, when you speak to them, what you said, where you have been, your purchases, photos, biometric data, even your notes to yourself – and all this dating back years. In 2014, the United States Supreme Court used this observation to justify the decision that police must obtain a warrant before rummaging through our smartphones. These devices ‘are now such a pervasive and insistent part of daily life that the proverbial visitor from Mars might conclude they were an important feature of human anatomy', as Chief Justice John Roberts observed in his written opinion . The Chief Justice probably wasn't making a metaphysical point – but the philosophers Andy Clark and David Chalmers were when they argued in ‘The Extended Mind' (1998) that technology is actually part of us. According to traditional cognitive science, ‘thinking' is a process of symbol manipulation or neural computation, which gets carried out by the brain. Clark and Chalmers broadly accept this computational theory of mind, but claim that tools can become seamlessly integrated into how we think. Objects such as smartphones or notepads are often just as functionally essential to our cognition as the synapses firing in our heads. They augment and extend our minds by increasing our cognitive power and freeing up internal resources. If accepted, the extended mind thesis threatens widespread cultural assumptions about the inviolate nature of thought, which sits at the heart of most legal and social norms. As the US Supreme Court declared in 1942: ‘freedom to think is absolute of its own nature; the most tyrannical government is powerless to control the inward workings of the mind.' This view has its origins in thinkers such as John Locke and René Descartes, who argued that the human soul is locked in a physical body, but that our thoughts exist in an immaterial world, inaccessible to other people. One's inner life thus needs protecting only when it is externalised, such as through speech . Many researchers in cognitive science still cling to this Cartesian conception – only, now, the private realm of thought coincides with activity in the brain. But today's legal institutions are straining against this narrow concept of the mind. They are trying to come to grips with how technology is changing what it means to be human, and to devise new normative boundaries to cope with this reality. Justice Roberts might not have known about the idea of the extended mind, but it supports his wry observation that smartphones have become part of our body. If our minds now encompass our phones, we are essentially cyborgs: part-biology, part-technology. Given how our smartphones have taken over what were once functions of our brains – remembering dates, phone numbers, addresses – perhaps the data they contain should be treated on a par with the information we hold in our heads. So if the law aims to protect mental privacy, its boundaries would need to be pushed outwards to give our cyborg anatomy the same protections as our brains. T his line of reasoning leads to some potentially radical conclusions. Some philosophers have argued that when we die, our digital devices should be handled as remains : if your smartphone is a part of who you are, then perhaps it should be treated more like your corpse than your couch. Similarly, one might argue that trashing someone's smartphone should be seen as a form of ‘extended' assault, equivalent to a blow to the head, rather than just destruction of property. If your memories are erased because someone attacks you with a club, a court would have no trouble characterising the episode as a violent incident. So if someone breaks your smartphone and wipes its contents, perhaps the perpetrator should be punished as they would be if they had caused a head trauma. The extended mind thesis also challenges the law's role in protecting both the content and the means of thought – that is, shielding what and how we think from undue influence. Regulation bars non-consensual interference in our neurochemistry (for example, through drugs), because that meddles with the contents of our mind. But if cognition encompasses devices, then arguably they should be subject to the same prohibitions. Perhaps some of the techniques that advertisers use to hijack our attention online, to nudge our decision-making or manipulate search results, should count as intrusions on our cognitive process. Similarly, in areas where the law protects the means of thought, it might need to guarantee access to tools such as smartphones – in the same way that freedom of expression protects people's right not only to write or speak, but also to use computers and disseminate speech over the internet. The courts are still some way from arriving at such decisions. Besides the headline-making cases of mass shooters, there are thousands of instances each year in which police authorities try to get access to encrypted devices. Although the Fifth Amendment to the US Constitution protects individuals' right to remain silent (and therefore not give up a passcode), judges in several states have ruled that police can forcibly use fingerprints to unlock a user's phone. (With the new facial-recognition feature on the iPhone X, police might only need to get an unwitting user to look at her phone.) These decisions reflect the traditional concept that the rights and freedoms of an individual end at the skin. But the concept of personal rights and freedoms that guides our legal institutions is outdated. It is built on a model of a free individual who enjoys an untouchable inner life. Now, though, our thoughts can be invaded before they have even been developed – and in a way, perhaps this is nothing new. The Nobel Prize-winning physicist Richard Feynman used to say that he thought with his notebook. Without a pen and pencil, a great deal of complex reflection and analysis would never have been possible. If the extended mind view is right, then even simple technologies such as these would merit recognition and protection as a part of the essential toolkit of the mind. Karina Vold This article was originally published at Aeon and has been republished under Creative Commons.","tags":"Psychology","url":"https://pelican-blog.netlify.com/are-you-just-inside-your-skin-or-is-your-smartphone-part-of-you.html"},{"title":"The tech bias: why Silicon Valley needs social theory","text":"In the summer of 2017, a now infamous memo came to light. Written by James Damore, then an engineer at Google, it claimed that the under-representation of women in tech was partly caused by inherent biological differences between men and women. The memo didn't offer any new evidence – on the contrary, it drew on longstanding sexist stereotypes that have been disproven time and again, and it included only the vaguest mention of decades of research in relevant domains such as gender studies. Given the expansive resources at Google, his omissions didn't stem from a lack of access to knowledge. Instead, they pointed to an unwillingness to accept that social theory is actually valid knowledge in the first place. That Google memo is an extreme example of an imbalance in how different ways of knowing are valued. Silicon Valley tech companies draw on innovative technical theory but have yet to really incorporate advances in social theory. The inattention to such knowledge becomes all too apparent when algorithms fail in their real-life applications – from automated soap-dispensers that fail to turn on when a user has dark brown skin, to the new iPhone X's inability to distinguish among different Asian women. Social theorists in fields such as sociology, geography, and science and technology studies have shown how race, gender and class biases inform technical design. So there's irony in the fact that employees hold sexist and racist attitudes, yet ‘we are supposed to believe that these same employees are developing \"neutral\" or \"objective\" decision-making tools', as the communications scholar Safiya Umoja Noble at the University of Southern California argues in her book Algorithms of Oppression (2018). In many cases, what's eroding the value of social knowledge is unintentional bias – on display when prominent advocates for equality in science and tech undervalue research in the social sciences. The physicist Neil DeGrasse Tyson, for example, has downplayed the link between sexism and under-representation in science. Apparently, he's happy to ignore extensive research pointing out that the natural sciences' male-dominated institutional cultures are a major cause of the attrition of female scientists at all stages of their careers. By contrast, social theorists have shown a keen interest in illuminating how unjust social relations inform the development of science and technology. In the 1980s, the anthropologist Lucy Suchman, now at Lancaster University in the UK, showed that even the employees at Xerox in California struggled to use the copy machines the company produced – leading them to laugh, mumble to themselves, and repeatedly ask questions such as: ‘Where's the start button?' The machines' designers had made an effort to write clear instructions, but people interpreted those guidelines in different ways, depending on factors that included their gender and class. The result was a machine that made sense to the engineers themselves, but didn't work so well for everyone else. Social theory also plays a critical role in understanding rare, catastrophic events, which can't be assessed solely in terms of technical failure. Human error and forms of social organisation – such as the hierarchies used to manage sensitive technologies – often play a critical role in whether or not a crisis is averted, as the sociologist Charles Perrow argues in Normal Accidents: Living with High-Risk Technologies (1984). For example, prior to the Challenger disaster in 1986 – in which a space shuttle exploded shortly after a take-off, killing all the crew-members on board – it turned out that some NASA staff had been aware of potential problems, caused by the material used to seal the rotating joints. However, certain organisational norms prevented these worries being transmitted to those who had the power to delay the launch. Because of a failure to appreciate the power dynamics behind NASA's structure, scientific knowledge could not prevent the crash, even when some of the scientists clearly saw the potential for disaster. T hese examples show that social theory is not about detaching oneself from the world, so as to observe it at a distance. Instead, its many practitioners often try to develop knowledge from the standpoint of engaged participants, questioning the limits of their own perspective. The goal is to improve knowledge of the social world, an effort that goes hand in hand with active efforts to change society for the better, while also thinking critically, and continuously, about what ‘better' means, and for whom. Detractors of social theory dislike it not because it's not effective, but because it is . It has catalysed profound shifts in race and gender relations in recent decades. In activist and community groups, people who were historically excluded from universities, such as people of colour and women, have been the pioneers of perspectives on the world that have an impact on everyday life for billions of people. In spite of these ongoing contributions, technical knowledge continues to be privileged over social knowledge. The disconnect is apparent in numerous ways, such as the salaries of researchers. While academic scientists tend to earn lower salaries than those in industry, scholars in the social sciences and the humanities earn less across the board than both groups – and have fewer grant and employment options overall, too. Science and tech are viewed as revenue-generating down the line, but the cost-saving benefits of improved social understanding, and the benefits that go beyond costs, tend to go underappreciated. Ironically, the same discriminatory systems targeted by social theory end up blocking underrepresented groups from getting a toehold in academia, the very seedbed of these ideas. Sexual harassment and racism are much more than individual incidents; they're institutionalised mechanisms for maintaining systemic barriers. As the feminist theorist Sara Ahmed points out in her book Living a Feminist Life (2017), sexual harassment is ‘a network that stops information from getting out. It is a set of alliances that come alive to stop something.' Such behaviour raises the stakes of fighting against the system, she says, by offering a choice to those who are on the receiving end: ‘get used to it, or get out' of the institution. ‘No wonder if these are the choices,' she notes, ‘many get out of it.' The skewed institutions that result should be everyone's concern. If tech companies are serious about building a better society, and aren't just paying lip service to justice for their own gain, they must attend more closely to social theory. If social insights were easy, and if practice followed readily from understanding, then racism, poverty and other debilitating systems of power and inequality would be a thing of the past. New insights about society are as challenging to produce as the most rarified scientific theorems – and addressing pressing contemporary problems requires as many kinds of knowers and ways of knowing as possible. Jess Bier This article was originally published at Aeon and has been republished under Creative Commons.","tags":"Technology","url":"https://pelican-blog.netlify.com/the-tech-bias-why-silicon-valley-needs-social-theory.html"},{"title":"The tech bias: Quantum cryptography is unbreakable. So is human ingenuity","text":"Two basic types of encryption schemes are used on the internet today. One, known as symmetric-key cryptography, follows the same pattern that people have been using to send secret messages for thousands of years. If Alice wants to send Bob a secret message, they start by getting together somewhere they can't be overheard and agree on a secret key; later, when they are separated, they can use this key to send messages that Eve the eavesdropper can't understand even if she overhears them. This is the sort of encryption used when you set up an online account with your neighbourhood bank; you and your bank already know private information about each other, and use that information to set up a secret password to protect your messages. The second scheme is called public-key cryptography, and it was invented only in the 1970s. As the name suggests, these are systems where Alice and Bob agree on their key, or part of it, by exchanging only public information. This is incredibly useful in modern electronic commerce: if you want to send your credit card number safely over the internet to Amazon, for instance, you don't want to have to drive to their headquarters to have a secret meeting first. Public-key systems rely on the fact that some mathematical processes seem to be easy to do, but difficult to undo. For example, for Alice to take two large whole numbers and multiply them is relatively easy; for Eve to take the result and recover the original numbers seems much harder. Public-key cryptography was invented by researchers at the Government Communications Headquarters (GCHQ) – the British equivalent (more or less) of the US National Security Agency (NSA) – who wanted to protect communications between a large number of people in a security organisation. Their work was classified, and the British government neither used it nor allowed it to be released to the public. The idea of electronic commerce apparently never occurred to them. A few years later, academic researchers at Stanford and MIT rediscovered public-key systems. This time they were thinking about the benefits that widespread cryptography could bring to everyday people, not least the ability to do business over computers. Now cryptographers think that a new kind of computer based on quantum physics could make public-key cryptography insecure. Bits in a normal computer are either 0 or 1. Quantum physics allows bits to be in a superposition of 0 and 1, in the same way that Schrödinger's cat can be in a superposition of alive and dead states. This sometimes lets quantum computers explore possibilities more quickly than normal computers. While no one has yet built a quantum computer capable of solving problems of nontrivial size (unless they kept it secret), over the past 20 years, researchers have started figuring out how to write programs for such computers and predict that, once built, quantum computers will quickly solve ‘hidden subgroup problems'. Since all public-key systems currently rely on variations of these problems, they could, in theory, be broken by a quantum computer. Cryptographers aren't just giving up, however. They're exploring replacements for the current systems, in two principal ways. One deploys quantum-resistant ciphers, which are ways to encrypt messages using current computers but without involving hidden subgroup problems. Thus they seem to be safe against code-breakers using quantum computers. The other idea is to make truly quantum ciphers. These would ‘fight quantum with quantum', using the same quantum physics that could allow us to build quantum computers to protect against quantum-computational attacks. Progress is being made in both areas, but both require more research, which is currently being done at universities and other institutions around the world. Yet some government agencies still want to restrict or control research into cryptographic security. They argue that if everyone in the world has strong cryptography, then terrorists, kidnappers and child pornographers will be able to make plans that law enforcement and national security personnel can't penetrate. But that's not really true. What is true is that pretty much anyone can get hold of software that, when used properly, is secure against any publicly known attacks. The key here is ‘when used properly'. In reality, hardly any system is always used properly. And when terrorists or criminals use a system incorrectly even once, that can allow an experienced codebreaker working for the government to read all the messages sent with that system. Law enforcement and national security personnel can put those messages together with information gathered in other ways – surveillance, confidential informants, analysis of metadata and transmission characteristics, etc – and still have a potent tool against wrongdoers. In his essay ‘A Few Words on Secret Writing' (1841), Edgar Allan Poe wrote: ‘[I]t may be roundly asserted that human ingenuity cannot concoct a cipher which human ingenuity cannot resolve.' In theory, he has been proven wrong: when executed properly under the proper conditions, techniques such as quantum cryptography are secure against any possible attack by Eve. In real-life situations, however, Poe was undoubtedly right. Every time an ‘unbreakable' system has been put into actual use, some sort of unexpected mischance eventually has given Eve an opportunity to break it. Conversely, whenever it has seemed that Eve has irretrievably gained the upper hand, Alice and Bob have found a clever way to get back in the game. I am convinced of one thing: if society does not give ‘human ingenuity' as much room to flourish as we can manage, we will all be poorer for it. The Mathematics of Secrets: Cryptography from Caesar Ciphers to Digital Encryption by Joshua Holden is out now through Princeton University Press. Joshua Holden This article was originally published at Aeon and has been republished under Creative Commons.","tags":"Social","url":"https://pelican-blog.netlify.com/quantum-cryptography-is-unbreakable-so-is-human-ingenuity.html"}]}